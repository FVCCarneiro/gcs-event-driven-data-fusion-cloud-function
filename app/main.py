#########################################################################################################
# main.py file for a Cloud function that triggers a Data Fusion pipeline when GCS file uploaded.
#########################################################################################################

# System
import os
import logging
# GCP token
import google.auth
from google.auth.transport import requests
# HTTP client
import requests as req
# Data
from json import dumps


def get_gcp_access_token():
    """
    Generate access token for Data Fusion authentication.
    The token is a JWT token generated by the google.auth package.
    Returns:
        creds.token: GCP JWT token to be placed in header.
    """
    # Generate google OAuth2 access and refresh tokens.
    creds, project = google.auth.default()

    # Populate credentials object by calling the refresh() method
    auth_req = requests.Request()
    creds.refresh(auth_req)

    return creds.token


def main(event, context):
    """
    Triggered by a change to a Cloud Storage bucket.
    Args:
         event (dict): Event payload.
         context (google.cloud.functions.Context): Metadata for the event.
    """

    logging.info(
        f'Cloud Function GCS trigger started.')

    # get environmental variables set in the inital configuraiton. assign them as local variables.
    PROJECT_ID = os.environ.get(
        'PROJECT_ID', 'Specified environment variable is not set.')
    INSTANCE_ID = os.environ.get(
        'INSTANCE_ID', 'Specified environment variable is not set.')
    REGION = os.environ.get(
        'REGION', 'Specified environment variable is not set.')
    NAMESPACE = os.environ.get(
        'NAMESPACE', 'Specified environment variable is not set.')
    TRIGGER_BUCKET = os.environ.get(
        'TRIGGER_BUCKET', 'Bucket that triggers the cloud function.')
    PIPELINE = os.environ.get(
        'PIPELINE', 'Name of the pipeline to trigger.')
    cdap_endpoint = os.environ.get(
        'CDAP_ENDPOINT', 'Endpoint of the data fusion instance to write secret to.')

    # GCS event manager
    bucket = event['bucket']
    blob_name = event['name']

    # check bucket is the specified bucket in runtime
    if bucket == TRIGGER_BUCKET:
        logging.info(
            f'File {blob_name} uploaded to bucket {bucket}.')

        # Get gcp JWT auth token
        gcp_token = get_gcp_access_token()

        # Set up pipeline macros (payload)
        data = {"gcs_file_name": blob_name}

        # Request headers
        headers = {
            "Authorization": f"Bearer {gcp_token}",
            "Content-Type": "application/json"
        }

        # Call users Data Fusion pipeline.
        pipeline_endpoint = f'namespaces/{NAMESPACE}/apps/{PIPELINE}/workflows/DataPipelineWorkflow/start'

        # Start pipeline via API
        response = req.post(
            url=cdap_endpoint + pipeline_endpoint,
            data=dumps(data),
            headers=headers)

        if response.status_code == 200:
            logging.info(
                f'Pipeline {PIPELINE} started.')
        else:
            logging.error(f'Failed to start pipeline')

    # File not uploaded to the intended bucket
    else:
        logging.info(
            f'File uploaded to bucket: {bucket}, no pipeline launched.')


if __name__ == '__main__':
    """
    If executed locally for testing.
    """
    logging.basicConfig(level=logging.INFO)

    event = {'bucket': 'trigger-bucket',
             'name': 'file.csv'}
    context = None

    # Cloud Function entry point
    main(event, context)
